{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def open_json_data(fileLocation = 'train-1.json'):\n",
    "    # Opening JSON file\n",
    "    openFile = open(fileLocation)\n",
    "    dictionary = json.load(openFile)\n",
    "    # return pd.DataFrame(dictionary)[:10]\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contribution_topic(data):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contribution_FieldOfStudy(data):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sparseDummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies from single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(data, dummy_column, delete_cat = True, spase= False):\n",
    "    ## create dummy\n",
    "    dummy = pd.get_dummies(data[dummy_column],prefix=dummy_column, drop_first=True, sparse=spase)\n",
    "    print('dummy', dummy.shape)\n",
    "    data = pd.concat([data, dummy], axis=1)\n",
    "    ## drop the original categorical column\n",
    "    if delete_cat : return data.drop(dummy_column, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_from_nestedList(data, dummy_column):\n",
    "    ## create dummy\n",
    "    print(data)\n",
    "    dummy = pd.get_dummies(data[dummy_column, 1].apply(pd.Series).stack()).sum(level=0)\n",
    "    print('dummy', dummy.shape)\n",
    "    data = pd.concat([data, dummy], axis=1)\n",
    "    ## drop the original categorical column\n",
    "    # if delete_cat : return data.drop(dummy_column, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the categorical years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(data, return_with_dummy= False):\n",
    "    conditions = [\n",
    "        (data['year'] < 2000),\n",
    "        (data['year'] >= 2000) & (data['year'] <= 2010),\n",
    "        (data['year'] > 2010) & (data['year'] < 2016),\n",
    "        (data['year'] >= 2016)\n",
    "    ]\n",
    "    values = [1, 2, 3, 4]\n",
    "    data['class_year'] = np.select(conditions, values)\n",
    "    if return_with_dummy : return dummy(data,\"class_year\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Categorical References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref(file, return_with_dummy= False):\n",
    "    conditions = [\n",
    "        (file['references'] == 0),\n",
    "        (file['references'] > 0) & (file['references'] <= 30),\n",
    "        (file['references'] > 30) & (file['references'] < 60),\n",
    "        (file['references'] >= 60)\n",
    "    ]\n",
    "    values = [1, 2, 3, 4]\n",
    "    file['class_ref'] = np.select(conditions, values)\n",
    "    if return_with_dummy : return dummy(file,\"class_ref\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_Nan(data):\n",
    "#     for column in data:\n",
    "#         if data[column].dtype == np.float64 or data[column].dtype == np.int64:data[column].fillna(data[column].mean())\n",
    "#     return data\n",
    "def fill_Nan(data):\n",
    "    data['year'] = data['year'].fillna(data['references'].mean())\n",
    "    data['references'] = data['references'].fillna(0)\n",
    "    data[\"fields_of_study\"] = data[\"fields_of_study\"].fillna(\"\")\n",
    "    data[\"title\"] = data[\"title\"].fillna(\"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titleLen(data):\n",
    "    data[\"title_len\"] = data.apply(lambda x:len(x[\"title\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AuthorsLen(data):\n",
    "    data[\"authors_len\"] = data.apply(lambda x:len(x[\"authors\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TopicsLen(data):\n",
    "    data[\"topics_len\"] = data.apply(lambda x:len(x[\"topics\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields of Study Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FieldsOfStudyLen(data):\n",
    "    data[\"fields_of_study_len\"] = data.apply(lambda x:len(x[\"fields_of_study\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find venue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_venue_in_dataFrame(venue, venues_dict):\n",
    "    venue_cat = venues_dict[venues_dict[\"venue\"].str.decode(\"utf-8\")  == venue]\n",
    "    if len(venue_cat)>0 : return venue_cat.iloc[0]['class_Venue']\n",
    "    return 0\n",
    "\n",
    "def get_venue(data):\n",
    "    venue_dictionary = pd.read_pickle(\"venue_data.pkl\")\n",
    "    data[\"venue_cat\"] = data.apply(lambda x:find_venue_in_dataFrame(x.venue, venue_dictionary),axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotY(Y):\n",
    "    Y = np.array(Y)\n",
    "    Y = np.sort(Y)\n",
    "    plt.plot(Y)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotY2(Ypred,Yreal):\n",
    "    length = len(Ypred)\n",
    "    plt.plot(Yreal,'b')\n",
    "    plt.plot(Ypred,'r')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = fill_Nan(data)\n",
    "    data = get_ref(data, True)\n",
    "    data = get_years(data, True)    \n",
    "    data = get_titleLen(data)\n",
    "    data = get_FieldsOfStudyLen(data)\n",
    "    data = data.assign(authorsLen2 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(authorsLen3 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = get_TopicsLen(data)\n",
    "    data = data.assign(authorsLen2 = lambda x: x['topics_len'] * x['topics_len'])\n",
    "    data = data.assign(authorsLen3 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len'])\n",
    "    data = get_AuthorsLen(data)\n",
    "    data = data.assign(authorsLen2 = lambda x: x['authors_len'] * x['authors_len'])\n",
    "    data = data.assign(authorsLen3 = lambda x: x['authors_len'] * x['authors_len'] * x['authors_len'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def eval(predicted, y_test):\n",
    "    ## Kpi\n",
    "    print(\"R2 (explained variance):\", round(metrics.r2_score(y_test, predicted), 2))\n",
    "    print(\"Mean Absolute Perc Error (Σ(|y-pred|/y)/n):\", round(np.mean(np.abs((y_test - predicted) / predicted)), 2))\n",
    "    print(\"Mean Absolute Error (Σ|y-pred|/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(y_test, predicted)))\n",
    "    print(\"Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)):\",\n",
    "            \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(y_test, predicted))))\n",
    "    ## residuals\n",
    "    residuals = y_test - predicted\n",
    "    max_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)\n",
    "    max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(\n",
    "        residuals).index(min(residuals))\n",
    "    # max_true, max_pred = y_test[max_idx], predicted[max_idx]\n",
    "    print(\"Max Error:\", \"{:,.0f}\".format(max_error))\n",
    "    return(residuals,max_error,max_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Throw everthing together\n",
    "The function will consist of two steps, first the program will load all the data and process this. This is necessary in order to know the relative contribution of different variables. Then it will split the data file in two sections, train and test in order to know the respective progress of the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy (9658, 3)\n",
      "dummy (9658, 3)\n",
      "(9658, 31)\n",
      "[-42.87578967 -30.83423084 -30.61915482 ... 271.90041362 287.24348446\n",
      " 329.89917235]\n",
      "[   0    0    0 ... 2887 3610 6764]\n",
      "R2 (explained variance): 0.62\n",
      "Mean Absolute Perc Error (Σ(|y-pred|/y)/n): 0.67\n",
      "Mean Absolute Error (Σ|y-pred|/n): 17\n",
      "Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)): 19\n",
      "Max Error: 87\n",
      "(array([42.87578967, 30.83423084, 30.61915482, ..., 84.47994865,\n",
      "       87.42824743, 86.89832411]), 87.42824742976137, 3086)\n",
      "0.019398206477996283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "def model():\n",
    "    \n",
    "    data = open_json_data()\n",
    "    \n",
    "    data = fill_Nan(data)\n",
    "    data = get_ref(data, True)\n",
    "    data = get_years(data, True)    \n",
    "    data = get_titleLen(data)\n",
    "    data = get_FieldsOfStudyLen(data)\n",
    "    data = get_TopicsLen(data)\n",
    "    data = get_AuthorsLen(data)\n",
    "    \n",
    "    data = data.assign(fields_of_study_len2 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(fields_of_study_len3 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(topicsLen2 = lambda x: x['topics_len'] * x['topics_len'])\n",
    "    data = data.assign(topicsLen3 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len'])\n",
    "    data = data.assign(topicsLen4 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len']* x['topics_len'])\n",
    "    data = data.assign(topicsLen5 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len']* x['topics_len']* x['topics_len'])\n",
    "    data = data.assign(authorsLen2 = lambda x: x['authors_len'] * x['authors_len'])\n",
    "    data = data.assign(authorsLen3 = lambda x: x['authors_len'] * x['authors_len'] * x['authors_len'])\n",
    "    data = data.assign(topicsFoS = lambda x: x['topics_len'] * x['fields_of_study_len'])\n",
    "    \n",
    "    print(data.shape)\n",
    "        \n",
    "    X = data.drop([\"citations\", \"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "    y = data[\"citations\"].values\n",
    "\n",
    "    # plotY(y)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=2021)\n",
    "    # model = make_pipeline(StandardScaler(), SGDRegressor(max_iter=10000, tol=1e-3))\n",
    "    # model = make_pipeline(StandardScaler(), linear_model.Ridge(alpha=1.1))\n",
    "    model = make_pipeline(StandardScaler(), linear_model.Lasso(alpha=1.1, selection='random'))\n",
    "    # model = make_pipeline(StandardScaler(), RandomForestRegressor(max_depth=2, random_state=0))\n",
    "    # model = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2,degree=2))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_val, y_val)\n",
    "    predictions = model.predict(X_val)\n",
    "\n",
    "    Ypred = np.array(predictions)\n",
    "    Ypred = np.sort(predictions)\n",
    "    Yreal = np.array(y_val)\n",
    "    Yreal = np.sort(y_val)\n",
    "    # plotY(Ypred)\n",
    "    # plotY2(Ypred, Yreal)\n",
    "\n",
    "    print(Ypred)\n",
    "    print(Yreal)\n",
    "    print(eval(Ypred[:-100], Yreal[:-100]))\n",
    "    print(score)\n",
    "    # print(score(y_val, predictions))\n",
    "    # print(model.score(X_val, y_val))\n",
    "    return \n",
    "    # huber = HuberRegressor().fit(X_train, y_train)\n",
    "    # print(huber.score(X_val, y_val))\n",
    "\n",
    "    # linear = LinearRegression()\n",
    "    # linear.fit(X_train, y_train)\n",
    "    # print(linear.score(X_val,y_val))\n",
    "\n",
    "    # return model\n",
    "    # return model.score(X_val, y_val)\n",
    "    # return predictions\n",
    "    # return model.fit(X_train, y_train).score(X_val, y_val)\n",
    "    \n",
    "    # return cross_val_score(model, X, y, cv=10)\n",
    "    # cross_val_score(model, X, y, cv=10)\n",
    "    # return cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9658, 17)\n",
      "(1000, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    3    4    4    6   17   20\n",
      "   22   26   29   33   40   43   43   48   49   50   52   54   54   56\n",
      "   57   58   58   61   65   67   68   70   72   75   77   78   80   82\n",
      "   82   83   83   83   85   86   88   90   91   91   91   92   93   96\n",
      "   97   98   99  100  100  101  102  103  104  104  107  108  108  109\n",
      "  110  110  112  112  113  113  114  114  115  115  115  116  117  117\n",
      "  118  119  119  120  120  120  120  121  121  121  122  122  122  122\n",
      "  123  123  123  124  124  125  125  126  126  127  127  127  128  130\n",
      "  131  131  131  131  132  132  133  133  133  133  134  135  136  136\n",
      "  137  138  139  140  140  140  140  141  143  143  145  145  145  145\n",
      "  145  146  146  146  146  149  149  149  151  151  151  151  151  152\n",
      "  152  153  153  153  154  155  155  155  156  156  156  157  158  158\n",
      "  158  158  159  159  160  160  161  161  161  161  161  161  162  162\n",
      "  162  162  162  162  163  163  163  164  164  164  165  166  166  166\n",
      "  168  168  168  169  169  170  170  170  171  171  171  171  171  172\n",
      "  172  172  172  172  173  173  173  174  174  175  175  176  176  176\n",
      "  176  177  177  177  178  178  178  179  179  179  179  180  180  180\n",
      "  180  180  180  180  180  180  181  181  182  183  183  184  184  185\n",
      "  185  185  185  186  186  187  188  188  188  188  188  188  189  189\n",
      "  190  190  190  190  191  191  192  192  192  192  192  193  194  194\n",
      "  194  194  195  195  196  196  196  197  197  197  197  197  198  198\n",
      "  198  198  199  199  199  199  199  201  201  201  201  202  202  203\n",
      "  203  204  204  204  204  205  205  205  206  206  207  208  208  208\n",
      "  208  209  209  209  210  210  210  211  211  211  211  213  213  213\n",
      "  213  213  214  214  214  214  214  215  215  215  216  216  216  216\n",
      "  217  217  217  218  218  218  219  219  219  219  220  220  220  221\n",
      "  221  221  221  221  221  221  221  222  222  222  222  223  223  223\n",
      "  223  223  224  224  224  225  225  226  227  227  228  228  228  228\n",
      "  229  229  229  229  229  229  229  230  230  230  231  231  231  231\n",
      "  232  232  232  233  233  233  233  233  234  234  234  234  234  235\n",
      "  235  235  235  236  236  237  238  238  238  238  238  239  240  240\n",
      "  240  240  240  241  241  241  241  242  242  242  242  242  243  243\n",
      "  243  244  244  245  245  246  246  246  246  248  249  249  249  249\n",
      "  249  250  250  250  252  252  252  253  253  253  254  254  255  255\n",
      "  255  256  256  256  257  257  257  257  257  257  257  258  258  259\n",
      "  260  260  260  260  261  261  261  263  264  264  265  266  267  267\n",
      "  270  271  271  271  271  272  272  273  273  273  273  273  274  274\n",
      "  274  275  275  275  276  276  276  277  277  277  278  278  279  279\n",
      "  279  280  280  280  281  281  282  282  283  283  284  284  286  287\n",
      "  288  288  289  290  291  291  291  291  292  292  292  293  293  293\n",
      "  293  294  294  294  294  294  295  295  296  297  298  299  299  300\n",
      "  300  301  301  301  302  303  303  305  305  305  306  307  307  307\n",
      "  308  310  310  310  310  310  311  311  311  312  312  312  312  315\n",
      "  315  316  316  317  317  317  318  318  318  319  319  319  320  321\n",
      "  321  321  323  323  324  324  325  325  325  326  326  327  327  327\n",
      "  327  327  328  328  328  329  329  329  329  331  331  332  332  332\n",
      "  332  332  333  334  334  334  334  334  335  335  335  335  335  335\n",
      "  336  336  336  337  337  337  337  338  338  338  339  340  340  340\n",
      "  340  341  341  343  343  343  344  344  345  345  346  346  346  346\n",
      "  347  347  347  348  348  349  349  349  349  350  350  350  350  350\n",
      "  351  351  351  351  351  351  352  352  353  353  355  355  356  356\n",
      "  357  359  360  363  363  363  363  364  364  365  369  369  369  370\n",
      "  370  371  372  372  373  375  375  376  376  376  376  376  377  378\n",
      "  378  379  380  380  381  381  382  382  383  384  384  385  385  386\n",
      "  386  386  387  388  388  389  391  392  392  392  393  394  394  395\n",
      "  396  397  397  398  398  398  400  401  401  401  402  403  403  403\n",
      "  403  404  404  405  407  407  408  409  409  409  410  410  411  413\n",
      "  413  413  414  414  415  416  416  417  417  419  420  420  422  423\n",
      "  424  424  425  425  425  427  427  428  428  428  430  430  432  433\n",
      "  433  434  434  434  435  435  437  438  438  438  440  440  440  441\n",
      "  443  444  445  445  445  447  449  449  449  449  454  454  455  456\n",
      "  456  456  456  456  457  461  462  464  466  466  469  470  471  471\n",
      "  472  474  476  476  476  476  477  478  479  479  479  479  480  480\n",
      "  480  483  484  487  488  489  495  496  498  499  499  500  504  504\n",
      "  504  506  513  514  518  520  520  521  526  530  530  531  533  534\n",
      "  535  536  540  543  543  545  547  551  552  552  557  557  558  562\n",
      "  563  564  571  573  574  576  577  579  579  583  584  588  590  590\n",
      "  590  595  596  596  597  606  607  610  611  614  623  625  628  630\n",
      "  631  635  643  647  671  672  676  679  682  684  693  696  704  724\n",
      "  732  748  751  812  824  848  883  892  903  931  983 1094 1102 1168\n",
      " 1208 1221 1686 1768 1770 2618]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "def model():\n",
    "    \n",
    "    data = open_json_data()\n",
    "    data = fill_Nan(data)\n",
    "    data = get_ref(data, False)\n",
    "    data = get_years(data, False)    \n",
    "    data = get_titleLen(data)\n",
    "    data = get_FieldsOfStudyLen(data)\n",
    "    data = get_TopicsLen(data)\n",
    "    data = get_AuthorsLen(data)\n",
    "    \n",
    "    print(data.shape)\n",
    "\n",
    "    data_test = open_json_data('test.json')\n",
    "    data_test = fill_Nan(data_test)\n",
    "    data_test = get_ref(data_test, False)\n",
    "    data_test = get_years(data_test, False)    \n",
    "    data_test = get_titleLen(data_test)\n",
    "    data_test = get_FieldsOfStudyLen(data_test)\n",
    "    data_test = get_TopicsLen(data_test)\n",
    "    data_test = get_AuthorsLen(data_test)\n",
    "\n",
    "    print(data_test.shape)\n",
    "\n",
    "    \n",
    "    X_train = data.drop([\"citations\", \"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "    y_train = data[\"citations\"].values\n",
    "    plotY(y_train)\n",
    "\n",
    "    X_test = data_test.drop([\"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), linear_model.Lasso(alpha=1.1))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    Ypred = pd.Series(predictions.astype(int))\n",
    "    Ypred = Ypred.where(Ypred > 0, 0)\n",
    "    \n",
    "    data_test[\"citations\"] = predictions.astype('int') \n",
    "    \n",
    "    # Ypred = np.array(Ypred)\n",
    "    YpredSorted = np.sort(Ypred)\n",
    "    print(YpredSorted)\n",
    "\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "model().to_json(\"output.json\", orient='records')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
