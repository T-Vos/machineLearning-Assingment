{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "In order to generate a bag of words representation we need the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set = np.array([ [x['venue'], x['citations']] for x in json.load(open('train-1.json'))[:3] if x['venue']])\n",
    "data_set = np.array([ [x['venue'], x['citations']] for x in json.load(open('train-1.json')) if x['venue']])\n",
    "# print(data_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion analysis of journals/venues\n",
    "Aim of the code is to divide the data in n equal portions, where the 'lowest' division accounts for the group of X that contributes the least to a given Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "def get_cat(dataFrame):\n",
    "    conditions = [\n",
    "        (dataFrame['cumsum'] == 0),\n",
    "        (dataFrame['cumsum'] > 0) & (dataFrame['cumsum'] <= 25),\n",
    "        (dataFrame['cumsum'] > 25) & (dataFrame['cumsum'] <= 50),\n",
    "        (dataFrame['cumsum'] > 50) & (dataFrame['cumsum'] <= 75),\n",
    "        (dataFrame['cumsum'] >= 75)\n",
    "    ]\n",
    "    values = [1, 2, 3, 4,5]\n",
    "    dataFrame['class_Venue'] = np.select(conditions, values)\n",
    "    return dataFrame\n",
    "\n",
    "def get_venue_dictionary(XY):\n",
    "    #split data \n",
    "    X, _ = np.split(XY, 2, 1)\n",
    "    \n",
    "    unique_X = np.unique(X,return_counts=True)\n",
    "    unique_X_length = len(unique_X[0])\n",
    "    print(unique_X_length)\n",
    "    zero_array = np.zeros((3,unique_X_length))\n",
    "\n",
    "    merge = [unique_X[0], unique_X[1].astype(int), zero_array[0].astype(int), zero_array[1].astype(int), zero_array[2].astype(float)]\n",
    "    \n",
    "    # Compute citations per topic\n",
    "    for x in XY :\n",
    "        merge[2][np.where(merge[0] == x[0])[0][0]] += int(x[1])\n",
    "\n",
    "    # Compute citations per topic divided by the amount of articles\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[3][i] = (merge[2][i]/merge[1][i])\n",
    "\n",
    "    # Compute new summed citations\n",
    "    summed_citations = np.sum(merge[3])\n",
    "    result = []\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[4][i] = 100/summed_citations*merge[3][i]\n",
    "        result.append((merge[0][i],merge[1][i],merge[2][i],merge[3][i],merge[4][i]))\n",
    "    \n",
    "    dtype = [('field_of_study', 'S100'), ('count', int), ('summed_citations', int), ('average_citations', float), ('contribution', float)]\n",
    "    structured_array = np.array(division, dtype=dtype)\n",
    "    sorted = np.sort(structured_array, order='contribution')\n",
    "    df = pd.DataFrame(sorted)\n",
    "    df['cumsum'] = df['contribution'].cumsum(axis=0)\n",
    "    df = get_cat(df)\n",
    "    # return {x['field_of_study'] : x['class_Venue'] for x in df}\n",
    "    return df\n",
    "    # return (df['field_of_study'],df['class_Venue'])\n",
    "    \n",
    "get_venue_dictionary(data_set).to_pickle(\"venue_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data_in_portions(XY):\n",
    "    #split data\n",
    "    XY_split = np.split(XY, 2, 1)\n",
    "    Y = XY_split[1].astype(np.int16)\n",
    "    \n",
    "    summed_Y = np.sum(Y)\n",
    "    print('There are in total', summed_Y, 'citations in this data')    \n",
    "    unique_X = np.unique(XY_split[0],return_counts=True)\n",
    "    print('Found',len(unique_X[0]), 'unique variables')\n",
    "    a = np.zeros((3,len(unique_X[0])))\n",
    "    c = [unique_X[0], unique_X[1].astype(int), a[0].astype(int), a[1].astype(int), a[2].astype(float)]\n",
    "    for x in XY :\n",
    "        c[2][np.where(c[0] == x[0])[0][0]] += int(x[1])\n",
    "    for (i, j) in enumerate(c[3]):\n",
    "        c[3][i] = (c[2][i]/c[1][i])\n",
    "        c[4][i] = 100/summed_Y*c[2][i]\n",
    "    return c\n",
    "    \n",
    "\n",
    "division = divide_data_in_portions(data_set)\n",
    "sorted_division = np.sort(division[3])\n",
    "\n",
    "print (division)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the line\n",
    "In order to make the data more acessible, the data can be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n",
      "     field_of_study  count  summed_citations  average_citations  contribution  \\\n",
      "0       b'ACL 1979'      1                 0                0.0      0.000000   \n",
      "1           b'ALVR'      7                 6                0.0      0.000000   \n",
      "2    b'AMERICASNLP'     15                 6                0.0      0.000000   \n",
      "3           b'BPPF'      3                 0                0.0      0.000000   \n",
      "4        b'CLPSYCH'      8                 2                0.0      0.000000   \n",
      "..              ...    ...               ...                ...           ...   \n",
      "294      b'ALW@ACL'      7               803              114.0      2.038262   \n",
      "295   b'EMNLP 2019'      1               144              144.0      2.574647   \n",
      "296        b'IJCAI'      1               169              169.0      3.021634   \n",
      "297   b'EMNLP 2017'      2               387              193.0      3.450742   \n",
      "298   b'SSST@EMNLP'      8              3659              457.0      8.170928   \n",
      "\n",
      "         cumsum  class_Venue  \n",
      "0      0.000000            1  \n",
      "1      0.000000            1  \n",
      "2      0.000000            1  \n",
      "3      0.000000            1  \n",
      "4      0.000000            1  \n",
      "..          ...          ...  \n",
      "294   82.782049            5  \n",
      "295   85.356696            5  \n",
      "296   88.378330            5  \n",
      "297   91.829072            5  \n",
      "298  100.000000            5  \n",
      "\n",
      "[299 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "division = divide_data_in_portions(data_set)\n",
    "dtype = [('field_of_study', 'S100'), ('count', int), ('summed_citations', int), ('average_citations', float), ('contribution', float)]\n",
    "\n",
    "# create a structured array\n",
    "structured_array = np.array(division, dtype=dtype)\n",
    "sorted = np.sort(structured_array, order='contribution')\n",
    "\n",
    "# plt.scatter(range(len(sorted_division)), sorted_division)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
