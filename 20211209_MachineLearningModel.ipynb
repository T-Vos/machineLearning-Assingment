{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def open_json_data(fileLocation = 'train-1.json'):\n",
    "    # Opening JSON file\n",
    "    openFile = open(fileLocation)\n",
    "    dictionary = json.load(openFile)\n",
    "    # return pd.DataFrame(dictionary)[:10]\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sparseDummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies from single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(data, dummy_column, delete_cat = True, spase= False):\n",
    "    ## create dummy\n",
    "    dummy = pd.get_dummies(data[dummy_column],prefix=dummy_column, drop_first=True, sparse=spase)\n",
    "    # print('dummy', dummy.shape)\n",
    "    data = pd.concat([data, dummy], axis=1)\n",
    "    ## drop the original categorical column\n",
    "    if delete_cat : return data.drop(dummy_column, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_from_nestedList(data, dummy_column):\n",
    "    ## create dummy\n",
    "    print(data)\n",
    "    dummy = pd.get_dummies(data[dummy_column, 1].apply(pd.Series).stack()).sum(level=0)\n",
    "    print('dummy', dummy.shape)\n",
    "    data = pd.concat([data, dummy], axis=1)\n",
    "    ## drop the original categorical column\n",
    "    # if delete_cat : return data.drop(dummy_column, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the categorical years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(data, return_with_dummy= False):\n",
    "    conditions = [\n",
    "        (data['year'] < 2000),\n",
    "        (data['year'] >= 2000) & (data['year'] <= 2010),\n",
    "        (data['year'] > 2010) & (data['year'] < 2016),\n",
    "        (data['year'] >= 2016)\n",
    "    ]\n",
    "    values = [1, 2, 3, 4]\n",
    "    data['class_year'] = np.select(conditions, values)\n",
    "    if return_with_dummy : return dummy(data,\"class_year\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Categorical References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref(file, return_with_dummy= False):\n",
    "    conditions = [\n",
    "        (file['references'] <12),\n",
    "        (file['references'] > 12) & (file['references'] <= 18),\n",
    "        (file['references'] > 18) & (file['references'] < 21),\n",
    "        (file['references'] > 21) & (file['references'] < 25),\n",
    "        (file['references'] > 25) & (file['references'] < 29),\n",
    "        (file['references'] > 29) & (file['references'] < 33),\n",
    "        (file['references'] > 33) & (file['references'] < 38),\n",
    "        (file['references'] > 38) & (file['references'] < 45),\n",
    "        (file['references'] > 45) & (file['references'] < 55),\n",
    "        (file['references'] >= 55)\n",
    "    ]\n",
    "    values = [1, 2, 3, 4, 5, 6 ,7 ,8,9,10]\n",
    "    file['class_ref'] = np.select(conditions, values)\n",
    "    if return_with_dummy : return dummy(file,\"class_ref\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_Nan(data):\n",
    "    data['year'] = data['year'].fillna(data['references'].mean())\n",
    "    data['references'] = data['references'].fillna(0)\n",
    "    data[\"fields_of_study\"] = data[\"fields_of_study\"].fillna(\"\")\n",
    "    data[\"title\"] = data[\"title\"].fillna(\"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length calculators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titleLen(data):\n",
    "    data[\"title_len\"] = data.apply(lambda x:len(x[\"title\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AuthorsLen(data):\n",
    "    data[\"authors_len\"] = data.apply(lambda x:len(x[\"authors\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TopicsLen(data):\n",
    "    data[\"topics_len\"] = data.apply(lambda x:len(x[\"topics\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields of Study Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FieldsOfStudyLen(data):\n",
    "    data[\"fields_of_study_len\"] = data.apply(lambda x:len(x[\"fields_of_study\"]),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Venue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Venue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "data_set = np.array([ [x['venue'], x['citations']] for x in json.load(open('train-1.json')) if x['venue']])\n",
    "\n",
    "def get_cat(dataFrame):\n",
    "    conditions = [\n",
    "        (dataFrame['cumsum'] == 0),\n",
    "        (dataFrame['cumsum'] > 0) & (dataFrame['cumsum'] <= 10),\n",
    "        (dataFrame['cumsum'] > 10) & (dataFrame['cumsum'] <= 20),\n",
    "        (dataFrame['cumsum'] > 20) & (dataFrame['cumsum'] <= 30),\n",
    "        (dataFrame['cumsum'] > 30) & (dataFrame['cumsum'] <= 40),\n",
    "        (dataFrame['cumsum'] > 40) & (dataFrame['cumsum'] <= 50),\n",
    "        (dataFrame['cumsum'] > 50) & (dataFrame['cumsum'] <= 60),\n",
    "        (dataFrame['cumsum'] > 60) & (dataFrame['cumsum'] <= 70),\n",
    "        (dataFrame['cumsum'] > 70) & (dataFrame['cumsum'] <= 80),\n",
    "        (dataFrame['cumsum'] > 80) & (dataFrame['cumsum'] <= 90),\n",
    "        (dataFrame['cumsum'] > 90) & (dataFrame['cumsum'] <= 92),\n",
    "        (dataFrame['cumsum'] > 92) & (dataFrame['cumsum'] <= 94),\n",
    "        (dataFrame['cumsum'] > 94) & (dataFrame['cumsum'] <= 96),\n",
    "        (dataFrame['cumsum'] > 96) & (dataFrame['cumsum'] <= 98),\n",
    "        (dataFrame['cumsum'] >= 98)\n",
    "    ]\n",
    "    values = [1,2,3,4,5,6,7,8,9,10,11,13,14,15,16]\n",
    "    dataFrame['class_Venue'] = np.select(conditions, values)\n",
    "    return dataFrame\n",
    "\n",
    "def get_venue_dictionary(XY):\n",
    "    #split data \n",
    "    X, _ = np.split(XY, 2, 1)\n",
    "    \n",
    "    unique_X = np.unique(X,return_counts=True)\n",
    "    unique_X_length = len(unique_X[0])\n",
    "    print(unique_X_length)\n",
    "    zero_array = np.zeros((3,unique_X_length))\n",
    "\n",
    "    merge = [unique_X[0], unique_X[1].astype(int), zero_array[0].astype(int), zero_array[1].astype(int), zero_array[2].astype(float)]\n",
    "    \n",
    "    # Compute citations per topic\n",
    "    for x in XY :\n",
    "        merge[2][np.where(merge[0] == x[0])[0][0]] += int(x[1])\n",
    "\n",
    "    # Compute citations per topic divided by the amount of articles\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[3][i] = (merge[2][i]/merge[1][i])\n",
    "\n",
    "    # Compute new summed citations\n",
    "    summed_citations = np.sum(merge[3])\n",
    "    result = []\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[4][i] = 100/summed_citations*merge[3][i]\n",
    "        result.append((merge[0][i],merge[1][i],merge[2][i],merge[3][i],merge[4][i]))\n",
    "    \n",
    "    dtype = [('venue', 'S100'), ('count', int), ('summed_citations', int), ('average_citations', float), ('contribution', float)]\n",
    "    \n",
    "    structured_array = np.array(result, dtype=dtype)\n",
    "    sorted = np.sort(structured_array, order='contribution')\n",
    "    df = pd.DataFrame(sorted)\n",
    "    df['cumsum'] = df['contribution'].cumsum(axis=0)\n",
    "    df = get_cat(df)\n",
    "    return df\n",
    "get_venue_dictionary(data_set).to_pickle(\"venue_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find venue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_venue_in_dataFrame(venue, venues_dict):\n",
    "    venue_cat = venues_dict[venues_dict[\"venue\"].str.decode(\"utf-8\")  == venue]\n",
    "    if len(venue_cat)>0 : return venue_cat.iloc[0]['class_Venue']\n",
    "    return 0\n",
    "\n",
    "def get_venue(data):\n",
    "    venue_dictionary = pd.read_pickle(\"venue_data.pkl\")\n",
    "    data[\"venue_cat\"] = data.apply(lambda x:find_venue_in_dataFrame(x.venue, venue_dictionary),axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Topic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-e226f2cb953d>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data_set = np.array([ [x['topics'], x['citations']] for x in json.load(open('train-1.json')) if x['topics']])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "data_set = np.array([ [x['topics'], x['citations']] for x in json.load(open('train-1.json')) if x['topics']])\n",
    "\n",
    "def get_cat(dataFrame):\n",
    "    conditions = [\n",
    "        (dataFrame['cumsum'] == 0),\n",
    "        (dataFrame['cumsum'] > 0) & (dataFrame['cumsum'] <= 10),\n",
    "        (dataFrame['cumsum'] > 10) & (dataFrame['cumsum'] <= 20),\n",
    "        (dataFrame['cumsum'] > 20) & (dataFrame['cumsum'] <= 30),\n",
    "        (dataFrame['cumsum'] > 30) & (dataFrame['cumsum'] <= 40),\n",
    "        (dataFrame['cumsum'] > 40) & (dataFrame['cumsum'] <= 50),\n",
    "        (dataFrame['cumsum'] > 50) & (dataFrame['cumsum'] <= 60),\n",
    "        (dataFrame['cumsum'] > 60) & (dataFrame['cumsum'] <= 70),\n",
    "        (dataFrame['cumsum'] > 70) & (dataFrame['cumsum'] <= 80),\n",
    "        (dataFrame['cumsum'] > 80) & (dataFrame['cumsum'] <= 90),\n",
    "        (dataFrame['cumsum'] > 90)\n",
    "    ]\n",
    "    values = [0,1, 2, 3, 4,5,6,7,8,9,10]\n",
    "    dataFrame['class_topic'] = np.select(conditions, values)\n",
    "    return dataFrame\n",
    "\n",
    "def createWordDictionaryPickle(wordsArray):\n",
    "    d = dict(enumerate(wordsArray))\n",
    "    a_file = open(\"topicsDictionary.pkl\", \"wb\")\n",
    "    pickle.dump(d, a_file)\n",
    "    a_file.close()\n",
    "    return\n",
    "\n",
    "def unique_Array_of_nested_array(x):\n",
    "    topics_array = list(map((lambda z: z[0]), x))\n",
    "    topics_array = np.concatenate(topics_array)\n",
    "    return np.unique(topics_array,return_counts=True)\n",
    "\n",
    "def get_venue_dictionary(XY):\n",
    "    #split data \n",
    "    X, _ = np.split(XY, 2, 1)\n",
    "\n",
    "    # Get unique topics\n",
    "    unique_topics = unique_Array_of_nested_array(X)\n",
    "    unique_topics_lenght = len(unique_topics[0])\n",
    "    \n",
    "    # Save as dictionary pickle\n",
    "    createWordDictionaryPickle(unique_topics[0])\n",
    "    \n",
    "\n",
    "    # Generate 0 arrays in order to fill them with the counts\n",
    "    zero_arrays = np.zeros((3,unique_topics_lenght))\n",
    "\n",
    "    # Merge arrays together\n",
    "    merge = [unique_topics[0], unique_topics[1].astype(int), zero_arrays[0].astype(int), zero_arrays[1].astype(int), zero_arrays[2].astype(float)]\n",
    "\n",
    "    # Compute citations per topic\n",
    "    for x in XY :\n",
    "        for topic in x[0]:\n",
    "            merge[2][np.where(merge[0] == topic)[0][0]] += int(x[1])\n",
    "\n",
    "\n",
    "    # Compute citations per topic divided by the amount of articles\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[3][i] = (merge[2][i]/merge[1][i])\n",
    "\n",
    "    # Compute new summed citations\n",
    "    summed_citations = np.sum(merge[3])\n",
    "\n",
    "    result = []\n",
    "    # Compute percentage of new total sum\n",
    "    for (i, j) in enumerate(merge[3]):\n",
    "        merge[4][i] = 100/summed_citations*merge[3][i]\n",
    "        result.append((merge[0][i],merge[1][i],merge[2][i],merge[3][i],merge[4][i]))\n",
    "\n",
    "    dtype = [('topic', 'str'), ('count', int), ('summed_citations', int), ('average_citations', float), ('contribution', float)]\n",
    "    structured_array = np.array(result, dtype=dtype)\n",
    "\n",
    "    sorted = np.sort(structured_array, order='contribution')\n",
    "    df = pd.DataFrame(sorted)\n",
    "    df['cumsum'] = df['contribution'].cumsum(axis=0)\n",
    "    df = get_cat(df)\n",
    "    return df\n",
    "get_venue_dictionary(data_set).to_pickle(\"topic_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract topic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicData(data):\n",
    "    a_file = open(\"topicsDictionary.pkl\", \"rb\")\n",
    "    d = pickle.load(a_file)\n",
    "    data[\"topics\"].to_dict(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def create_countVector():\n",
    "    train_data = [ x['abstract'] for x in json.load(open('train-1.json')) if x['abstract']]\n",
    "    tfIdfTransformer = TfidfTransformer(use_idf=True)\n",
    "    countVectorizer = CountVectorizer()\n",
    "    wordCount = countVectorizer.fit_transform(train_data)\n",
    "    newTfIdf = tfIdfTransformer.fit_transform(wordCount)\n",
    "    df = pd.DataFrame(newTfIdf[0].T.todense(), index = countVectorizer.get_feature_names(), columns = [\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending = False)\n",
    "\n",
    "create_countVector()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotY(Y):\n",
    "    Y = np.array(Y)\n",
    "    Y = np.sort(Y)\n",
    "    plt.plot(Y)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plotY2(Ypred,Yreal):\n",
    "    length = len(Ypred)\n",
    "    plt.plot(Yreal,'b')\n",
    "    plt.plot(Ypred,'r')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotHistY(Y):\n",
    "    plt.hist(x=Y, bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def eval(predicted, y_test):\n",
    "    ## Kpi\n",
    "    print(\"R2 (explained variance):\", round(metrics.r2_score(y_test, predicted), 2))\n",
    "    print(\"Mean Absolute Perc Error (Σ(|y-pred|/y)/n):\", round(np.mean(np.abs((y_test - predicted) / predicted)), 2))\n",
    "    print(\"Mean Absolute Error (Σ|y-pred|/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(y_test, predicted)))\n",
    "    print(\"Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)):\",\n",
    "            \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(y_test, predicted))))\n",
    "    ## residuals\n",
    "    residuals = y_test - predicted\n",
    "    max_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)\n",
    "    max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(\n",
    "        residuals).index(min(residuals))\n",
    "    # max_true, max_pred = y_test[max_idx], predicted[max_idx]\n",
    "    print(\"Max Error:\", \"{:,.0f}\".format(max_error))\n",
    "    return(residuals,max_error,max_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR,LinearSVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostRegressor,VotingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def modelCalculators(X_train, y_train, X_val, y_val, train=True):\n",
    "    \n",
    "    sdgRegres = make_pipeline(StandardScaler(), SGDRegressor(max_iter=100000, tol=1e-3))\n",
    "\n",
    "    if train:\n",
    "        sdgRegres.fit(X_train, y_train)\n",
    "        sdgRegres_score = sdgRegres.score(X_val, y_val)\n",
    "        # sdgRegres_transformed = sdgRegres.predict(X_val)\n",
    "        # sdgRegres_pred = np.exp(sdgRegres_transformed)\n",
    "        print('SDG',sdgRegres_score)\n",
    "\n",
    "\n",
    "    linearRidg = make_pipeline(StandardScaler(), linear_model.Ridge(alpha=1.1))\n",
    "\n",
    "    if train:\n",
    "        linearRidg.fit(X_train, y_train)\n",
    "        linearRidg_score = linearRidg.score(X_val, y_val)\n",
    "        # linearRidg_transformed = linearRidg.predict(X_val)\n",
    "        # linearRidg_pred = np.exp(linearRidg_transformed)\n",
    "        print('Ridg',linearRidg_score)\n",
    "    \n",
    "    linearLasso = make_pipeline(StandardScaler(), linear_model.Lasso(alpha=1.1, selection='random'))\n",
    "\n",
    "    if train:\n",
    "        linearLasso.fit(X_train, y_train)\n",
    "        linearLasso_score = linearLasso.score(X_val, y_val)\n",
    "        # linearLasso_transformed = linearLasso.predict(X_val)\n",
    "        # linearLasso_pred = np.exp(linearLasso_transformed)\n",
    "        print('Lasso',linearLasso_score)\n",
    "\n",
    "    svrRegress = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2, kernel='rbf', degree=4))\n",
    "\n",
    "    if train:\n",
    "        svrRegress.fit(X_train, y_train)\n",
    "        svrRegress_score = svrRegress.score(X_val, y_val)\n",
    "        # svrRegress_transformed = svrRegress.predict(X_val)\n",
    "        # svrRegress_pred = np.exp(svrRegress_transformed)\n",
    "        print('SVR Regress',svrRegress_score)\n",
    "\n",
    "    linearSVR = make_pipeline(StandardScaler(), LinearSVR(random_state=0,max_iter=100000, tol=1e-5))\n",
    "\n",
    "    if train:\n",
    "        linearSVR.fit(X_train, y_train)\n",
    "        linearSVR_score = linearSVR.score(X_val, y_val)\n",
    "        # linearSVR_transformed = linearSVR.predict(X_val)\n",
    "        # linearSVR_pred = np.exp(linearSVR_transformed)\n",
    "        print('SVR Lin',linearSVR_score)\n",
    "\n",
    "    adaBoostReg = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "    if train:\n",
    "        adaBoostReg.fit(X_train, y_train)\n",
    "        adaBoostReg_score = adaBoostReg.score(X_val, y_val)\n",
    "        adaBoostReg_transformed = adaBoostReg.predict(X_val)\n",
    "        # adaBoostReg_pred = np.exp(adaBoostReg_transformed)\n",
    "        print('Ada',adaBoostReg_score)\n",
    "\n",
    "    regressionVoted = VotingRegressor([('Ridg', linearRidg),('SVR', svrRegress),('ada', adaBoostReg), ('linearSVR', linearSVR)],n_jobs=-1)\n",
    "    regressionVoted.fit(X_train, y_train)\n",
    "    \n",
    "    if train : \n",
    "        regressionVoted_score = regressionVoted.score(X_val, y_val)\n",
    "        print('Voted',regressionVoted_score)\n",
    "\n",
    "    prediction = regressionVoted.predict(X_val)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(data):\n",
    "    data = fill_Nan(data)\n",
    "    data = get_ref(data, True)\n",
    "    data = get_years(data, True)    \n",
    "    data = get_titleLen(data)\n",
    "    data = get_FieldsOfStudyLen(data)\n",
    "    data = get_TopicsLen(data)\n",
    "    data = get_AuthorsLen(data)\n",
    "    data = get_venue(data)\n",
    "    data = getTopicData(data)\n",
    "    print(data)\n",
    "    data = data.assign(fields_of_study_len2 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(fields_of_study_len3 = lambda x: x['fields_of_study_len'] * x['fields_of_study_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(topicsLen2 = lambda x: x['topics_len'] * x['topics_len'])\n",
    "    data = data.assign(topicsLen3 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len'])\n",
    "    data = data.assign(topicsLen4 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len']* x['topics_len'])\n",
    "    data = data.assign(topicsLen5 = lambda x: x['topics_len'] * x['topics_len'] * x['topics_len']* x['topics_len']* x['topics_len'])\n",
    "    data = data.assign(authorsLen2 = lambda x: x['authors_len'] * x['authors_len'])\n",
    "    data = data.assign(topicsFoS = lambda x: x['topics_len'] * x['fields_of_study_len'])\n",
    "    data = data.assign(topicsVenueCat = lambda x: x['topics_len'] * x['venue_cat'])\n",
    "    data = data.assign(references2 = lambda x: x['references'] * x['references'])\n",
    "    data = data.assign(references3 = lambda x: x['references'] * x['references']* x['references'])\n",
    "    data = data.assign(references4 = lambda x: x['references'] * x['references']* x['references']* x['references'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_x(data, train=True):\n",
    "    if train : return data.drop([\"citations\", \"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "    return data.drop([\"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The function will consist of two steps, first the program will load all the data and process this. This is necessary in order to know the relative contribution of different variables. Then it will split the data file in two sections, train and test in order to know the respective progress of the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 doi  \\\n",
      "0                10.3115/v1/P15-1039   \n",
      "1     10.18653/v1/2020.eval4nlp-1.12   \n",
      "2               10.18653/v1/W17-3516   \n",
      "3               10.18653/v1/S17-2160   \n",
      "4               10.18653/v1/W15-2205   \n",
      "...                              ...   \n",
      "9653             10.3115/v1/W14-0202   \n",
      "9654  10.26615/978-954-452-058-8_001   \n",
      "9655     10.18653/V1/2021.SMM4H-1.16   \n",
      "9656      10.18653/v1/2021.case-1.22   \n",
      "9657         10.3115/1075218.1075259   \n",
      "\n",
      "                                                  title  \\\n",
      "0     Generating High Quality Proposition Banks for ...   \n",
      "1     One of these words is not like the other: a re...   \n",
      "2     The Code2Text Challenge: Text Generation in So...   \n",
      "3     The Meaning Factory at SemEval-2017 Task 9: Pr...   \n",
      "4               Semantic Parsing for Textual Entailment   \n",
      "...                                                 ...   \n",
      "9653  IBM’s Belief Tracker: Results On Dialog State ...   \n",
      "9654  RANLP 2019 Multilingual Headline Generation Ta...   \n",
      "9655  A Joint Training Approach to Tweet Classificat...   \n",
      "9656  Team “DaDeFrNi” at CASE 2021 Task 1: Document ...   \n",
      "9657  Headline Generation Based on Statistical Trans...   \n",
      "\n",
      "                                               abstract  \\\n",
      "0     Semantic role labeling (SRL) is crucial to nat...   \n",
      "1     Word embeddings are an active topic in the NLP...   \n",
      "2     We propose a new shared task for tactical data...   \n",
      "3     We evaluate a semantic parser based on a chara...   \n",
      "4     In this paper we gauge the utility of general-...   \n",
      "...                                                 ...   \n",
      "9653  Accurate dialog state tracking is crucial for ...   \n",
      "9654  The objective of the 2019 RANLP Multilingual H...   \n",
      "9655  In this work we describe our submissions to th...   \n",
      "9656  This paper accompanies our top-performing subm...   \n",
      "9657  Extractive summarization techniques cannot gen...   \n",
      "\n",
      "                                                authors        venue    year  \\\n",
      "0     [A. Akbik, Laura Chiticariu, Marina Danilevsky...          ACL  2015.0   \n",
      "1     [Jesper Brink Andersen, Mikkel Bak Bertelsen, ...     EVAL4NLP  2020.0   \n",
      "2           [Kyle Richardson, Sina Zarrieß, Jonas Kuhn]         INLG  2017.0   \n",
      "3                            [Rik van Noord, Johan Bos]  SemEval@ACL  2017.0   \n",
      "4                     [Elisabeth Lien, Milen Kouylekov]         IWPT  2015.0   \n",
      "...                                                 ...          ...     ...   \n",
      "9653  [Rudolf Kadlec, Jindřich Libovický, Jan Macek,...      DM@EACL  2014.0   \n",
      "9654  [Marina Litvak, John M. Conroy, Peter A. Ranke...               2019.0   \n",
      "9655                         [Mohab Elkaref, L. Hassan]        SMM4H  2021.0   \n",
      "9656  [Francesco Re, D. Végh, Dennis Atzenhofer, Nik...         CASE  2021.0   \n",
      "9657            [Michele Banko, V. Mittal, M. Witbrock]          ACL  2000.0   \n",
      "\n",
      "      references                                             topics  \\\n",
      "0             39                           [Semantic role labeling]   \n",
      "1             44                                                 []   \n",
      "2             30  [Natural language generation, Library (computi...   \n",
      "3             11  [Parsing, Convolutional neural network, Text-b...   \n",
      "4             26  [Textual entailment, Parsing, SemEval, Semanti...   \n",
      "...          ...                                                ...   \n",
      "9653          25  [Dialog system, Discriminative model, Inclusio...   \n",
      "9654          18                [Wikipedia, Heuristic, Information]   \n",
      "9655          12                                                 []   \n",
      "9656          15                                                 []   \n",
      "9657          38  [Statistical machine translation, Approximatio...   \n",
      "\n",
      "      is_open_access     fields_of_study  ...  class_ref_9  class_ref_10  \\\n",
      "0               True  [Computer Science]  ...            0             0   \n",
      "1               True  [Computer Science]  ...            0             0   \n",
      "2               True  [Computer Science]  ...            0             0   \n",
      "3               True  [Computer Science]  ...            0             0   \n",
      "4               True  [Computer Science]  ...            0             0   \n",
      "...              ...                 ...  ...          ...           ...   \n",
      "9653            True  [Computer Science]  ...            0             0   \n",
      "9654            True  [Computer Science]  ...            0             0   \n",
      "9655           False  [Computer Science]  ...            0             0   \n",
      "9656           False                      ...            0             0   \n",
      "9657            True  [Computer Science]  ...            0             0   \n",
      "\n",
      "      class_year_2  class_year_3  class_year_4  title_len  \\\n",
      "0                0             1             0         81   \n",
      "1                0             0             1        124   \n",
      "2                0             0             1         60   \n",
      "3                0             0             1         87   \n",
      "4                0             1             0         39   \n",
      "...            ...           ...           ...        ...   \n",
      "9653             0             1             0         73   \n",
      "9654             0             0             1         57   \n",
      "9655             0             0             1        112   \n",
      "9656             0             0             1        101   \n",
      "9657             1             0             0         52   \n",
      "\n",
      "      fields_of_study_len  topics_len  authors_len  venue_cat  \n",
      "0                       1           1            6          8  \n",
      "1                       1           0            5          2  \n",
      "2                       1           5            3          3  \n",
      "3                       1           6            2          3  \n",
      "4                       1          23            2          2  \n",
      "...                   ...         ...          ...        ...  \n",
      "9653                    1          11            4          2  \n",
      "9654                    1           3            4          0  \n",
      "9655                    1           0            2          2  \n",
      "9656                    0           0            4          2  \n",
      "9657                    1           3            3          8  \n",
      "\n",
      "[9658 rows x 29 columns]\n",
      "(9658, 41)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0844b38c1467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-0844b38c1467>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_val_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpredY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelCalculators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_transformed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# pred_y = modelCalculators(X_train, y_train, X_val, y_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f8def5446f76>\u001b[0m in \u001b[0;36mmodelCalculators\u001b[1;34m(X_train, y_train, X_val, y_val, train)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0msdgRegres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0msdgRegres_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msdgRegres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# sdgRegres_transformed = sdgRegres.predict(X_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \"\"\"\n\u001b[0;32m    765\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[0;32m    767\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def model(): \n",
    "    data = open_json_data()\n",
    "\n",
    "    data = transformData(data)\n",
    "    print(data.shape)\n",
    "            \n",
    "    X = return_x(data)\n",
    "    y = data[\"citations\"].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=2000)\n",
    "    \n",
    "    y_train_transformed = np.log1p(y_train)\n",
    "    y_val_transformed = np.log1p(y_val)\n",
    "\n",
    "    predY = modelCalculators(X_train, y_train_transformed, X_val, y_val_transformed)\n",
    "    # pred_y = modelCalculators(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    pred_y = np.exp(predY)\n",
    "\n",
    "    plotY(pred_y)\n",
    "    plotY(y_val)\n",
    "    plotY2(pred_y, y_val)\n",
    "    eval(pred_y, y_val)\n",
    "    YpredSorted = np.sort(pred_y)\n",
    "    print(YpredSorted)\n",
    "    return\n",
    "\n",
    "model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9658, 39)\n",
      "(1000, 38)\n",
      "[  4   6   8   8   8  10  10  10  10  14  14  15  15  18  19  19  20  20\n",
      "  20  22  22  24  25  27  27  27  28  29  29  30  31  32  33  34  35  36\n",
      "  36  37  37  38  39  40  40  42  42  42  43  46  46  47  47  47  47  49\n",
      "  49  50  50  50  51  51  52  53  53  54  54  54  55  55  55  56  56  56\n",
      "  56  57  57  57  57  57  57  58  58  58  59  59  60  60  61  61  61  62\n",
      "  62  62  62  62  63  63  64  64  64  64  64  64  64  64  64  64  64  64\n",
      "  65  65  65  65  65  65  66  66  66  66  67  67  67  67  67  68  68  68\n",
      "  68  68  68  69  69  69  69  69  69  69  70  70  70  70  70  70  70  70\n",
      "  71  71  71  71  71  72  72  72  72  72  72  72  72  73  73  73  73  73\n",
      "  73  73  73  73  73  73  73  73  73  74  74  74  74  74  75  75  75  75\n",
      "  75  75  76  76  76  76  76  76  76  76  76  76  76  77  77  77  77  77\n",
      "  77  77  77  77  77  77  77  77  77  78  78  78  78  78  78  78  78  78\n",
      "  78  79  79  79  79  79  79  79  79  79  79  79  79  79  79  80  80  80\n",
      "  80  80  80  81  81  81  81  81  81  81  81  81  81  81  81  81  81  82\n",
      "  82  82  82  82  82  82  82  82  82  82  83  83  83  83  83  83  84  84\n",
      "  84  84  84  84  84  84  84  84  84  84  84  84  84  84  84  84  84  84\n",
      "  84  85  85  85  85  85  85  85  85  85  85  85  85  86  86  86  86  86\n",
      "  86  86  86  86  86  86  86  86  86  86  86  86  86  86  87  87  87  87\n",
      "  87  87  87  87  87  87  87  87  87  87  87  87  87  88  88  88  88  88\n",
      "  88  88  88  89  89  89  89  89  89  89  89  89  89  90  90  90  90  90\n",
      "  90  90  90  90  90  91  91  91  91  91  91  91  91  91  91  91  91  91\n",
      "  91  91  91  91  91  91  92  92  92  92  92  92  92  92  92  92  92  92\n",
      "  92  92  93  93  93  93  93  93  93  93  93  93  93  93  93  93  93  93\n",
      "  93  93  94  94  94  94  94  94  94  94  94  95  95  95  95  95  95  95\n",
      "  95  96  96  96  96  97  97  97  97  97  97  97  97  97  97  97  98  98\n",
      "  98  98  98  98  98  98  98  99  99  99  99  99  99  99  99  99  99  99\n",
      "  99 100 100 100 100 100 100 100 101 101 101 101 101 101 101 101 101 101\n",
      " 101 101 101 101 101 101 102 102 102 102 102 102 102 102 102 103 103 103\n",
      " 103 103 103 103 103 103 103 103 104 104 104 104 104 104 104 105 105 105\n",
      " 105 105 106 106 106 106 106 106 106 106 106 106 106 106 106 106 106 107\n",
      " 107 107 107 107 107 107 108 108 108 108 108 108 108 108 108 108 108 108\n",
      " 108 108 108 108 109 109 109 109 109 109 109 109 109 109 110 110 110 111\n",
      " 111 111 111 111 111 111 111 111 111 111 111 111 111 111 112 112 112 112\n",
      " 112 112 112 113 113 113 113 113 113 114 114 114 114 114 114 114 114 115\n",
      " 115 115 115 115 116 116 116 116 116 116 116 116 116 116 117 117 117 117\n",
      " 117 117 117 117 117 118 118 118 118 118 118 118 118 118 118 118 119 119\n",
      " 119 119 119 119 119 119 119 119 119 119 120 120 120 120 120 121 121 121\n",
      " 121 121 121 121 121 121 121 122 122 122 122 122 122 122 122 123 123 123\n",
      " 123 123 123 123 123 123 124 124 124 124 124 124 125 125 125 125 125 125\n",
      " 126 126 126 126 126 126 126 126 126 127 127 127 127 127 127 127 127 127\n",
      " 127 127 128 128 128 128 128 128 128 129 129 129 129 129 129 129 130 130\n",
      " 130 130 130 130 130 130 130 131 131 131 131 131 131 132 132 132 133 133\n",
      " 133 133 133 134 134 134 134 134 135 135 135 135 135 135 135 135 135 136\n",
      " 136 136 137 137 137 137 137 137 137 137 137 137 137 138 138 138 138 138\n",
      " 138 139 139 139 139 139 140 140 140 140 140 140 140 140 140 140 140 141\n",
      " 141 141 141 141 142 142 142 142 142 143 143 144 144 144 144 145 145 145\n",
      " 145 146 146 146 146 146 147 147 147 147 148 149 149 149 149 149 150 150\n",
      " 151 151 151 151 151 153 153 153 153 154 154 154 155 155 155 155 155 155\n",
      " 156 156 156 156 156 157 157 157 157 157 157 158 158 158 159 159 160 160\n",
      " 161 162 162 162 163 163 163 164 164 164 164 164 164 165 165 165 165 165\n",
      " 165 165 166 166 168 168 169 169 171 172 172 172 172 173 173 173 174 175\n",
      " 175 177 177 177 177 178 178 178 179 179 179 181 181 181 181 181 182 182\n",
      " 182 183 185 186 186 187 187 188 190 191 192 196 196 196 196 198 198 200\n",
      " 200 201 201 202 202 204 204 208 208 210 211 211 212 212 213 213 214 215\n",
      " 216 217 219 222 235 235 236 245 247 249 249 250 253 260 275 289 307 317\n",
      " 325 332 333 338 436 438 443 462 481 718]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(): \n",
    "    data = open_json_data()\n",
    "    data = transformData(data)\n",
    "    print(data.shape)\n",
    "\n",
    "    data_test = open_json_data('test.json')\n",
    "    data_test = transformData(data_test)\n",
    "    print(data_test.shape)\n",
    "  \n",
    "    data_test_final = open_json_data('test.json')\n",
    "    \n",
    "    X = return_x(data)\n",
    "    y = data[\"citations\"].values\n",
    "    # y_transformed = np.log1p(y)\n",
    "    X_test = return_x(data_test, False)\n",
    "\n",
    "    predictions = modelCalculators(X, y, X_test, [], False)\n",
    "\n",
    "    Ypred = pd.Series(predictions.astype(int))\n",
    "    Ypred = Ypred.where(Ypred > 0, 0)\n",
    "    \n",
    "    data_test_final[\"citations\"] = Ypred.astype('int')\n",
    "\n",
    "    YpredSorted = np.sort(Ypred)\n",
    "    print(YpredSorted)\n",
    "    plotY(Ypred)\n",
    "   \n",
    "    return data_test_final.drop([\"year\", \"references\", \"is_open_access\",\"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1)\n",
    "\n",
    "model().to_json(\"predicted.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9658, 17)\n",
      "(1000, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVaElEQVR4nO3dbYxc133f8e9/Z/kkUpRIi1IormRSAZOUFuJaYhXJLowiSirWMUyjsFEGcMUWMgioSuukBQKxQRP0hQC3DYxEaK1WsBPTjWOFVYSINaLYAu2gSOpKXkV29UCxYixZ3IgWqYfIJGUuuTv/vpiz5NVyZncoLXfP7n4/wGDunHvunXNmd+e399xzZyIzkSSpm4G5boAkqV6GhCSpJ0NCktSTISFJ6smQkCT1NDjXDZjOFVdckRs3bpzrZkjSvPLEE0+8mpnr3u1+qg+JjRs3Mjw8PNfNkKR5JSJ+MBP7cbhJktSTISFJ6smQkCT1ZEhIknoyJCRJPRkSkqSeDAlJUk+GhCRV5rHvv8bnvnGQsfH2XDfFkJCk2gz/4A3u/eYhxiv4vh9DQpLUkyEhSerJkJCkSgUx100wJCRJvRkSklSZrOCE9QRDQpIqFXM/2mRISJJ6MyQkqTIVjTYZEpJUqwpGmwwJSVJvhoQkVaai0SZDQpJqFRVMbzIkJEk9GRKSVBlnN0mSpjX3g02GhCRpCoaEJFUmK5rfZEhIUqUqmNxkSEiSejMkJKky8252U0T8WkQ8ExFPR8RXI2J5RKyNiEcj4vlyv6ZRf3dEHIqIgxFxW6P8xoh4qqy7N2q4UkSSKlXDW+S0IRERG4B/BWzNzOuBFrADuBvYn5mbgf3lMRGxpax/H7AN+HxEtMru7gN2AZvLbduM9kaSNKP6HW4aBFZExCBwCfAysB3YU9bvAT5elrcDD2TmaGa+ABwCboqI9cDqzPx2dr526cuNbSRJRUWjTdOHRGb+DfDbwEvAEeDNzPwGcFVmHil1jgBXlk02AIcbuxgpZRvK8uTy80TErogYjojhY8eOXViPJEkzpp/hpjV0jg42AVcDKyPiU1Nt0qUspyg/vzDz/szcmplb161bN10TJUkXST/DTb8AvJCZxzLzDPAQ8EHglTKERLk/WuqPANc0th+iMzw1UpYnl0uSmiqa3tRPSLwE3BwRl5TZSLcCB4B9wM5SZyfwcFneB+yIiGURsYnOCerHy5DU8Yi4uezn9sY2kqSGCiY2AZ0T0lPKzMci4kHgr4Ax4EngfmAVsDci7qATJJ8s9Z+JiL3As6X+XZk5XnZ3J/AlYAXwSLlJkio1bUgAZOZvAb81qXiUzlFFt/r3APd0KR8Grr/ANkrSolLPYJNXXEtSlSoZbTIkJEm9GRKSVJmKJjcZEpJUoxo+twkMCUnSFAwJSaqM30wnSZpSHYNNhoQkaQqGhCRVxtlNkqQpVTK5yZCQJPVmSEhSZSoabTIkJKlGUcn8JkNCktSTISFJlXF2kyRpanWMNhkSkqTeDAlJqoyf3SRJmlIlo02GhCSpN0NCkmpTz2iTISFJNfKzmyRJ1TMkJKkyFY02GRKSVCM/u0mSVD1DQpIqkxV9eJMhIUkVcnaTJKl6hoQkVaai0SZDQpJqVMlokyEhSerNkJCkylQ02mRISFKNopLpTX2FRERcHhEPRsRzEXEgIm6JiLUR8WhEPF/u1zTq746IQxFxMCJua5TfGBFPlXX3Ri2vgiSpq36PJH4X+LPM/Bng/cAB4G5gf2ZuBvaXx0TEFmAH8D5gG/D5iGiV/dwH7AI2l9u2GeqHJC0Y82p2U0SsBj4MfBEgM09n5t8C24E9pdoe4ONleTvwQGaOZuYLwCHgpohYD6zOzG9n53LCLze2kSQ11DLM0s+RxHXAMeD3I+LJiPhCRKwErsrMIwDl/spSfwNwuLH9SCnbUJYnl58nInZFxHBEDB87duyCOiRJmjn9hMQgcANwX2Z+ADhJGVrqoVsA5hTl5xdm3p+ZWzNz67p16/pooiQtHFnR/KZ+QmIEGMnMx8rjB+mExitlCIlyf7RR/5rG9kPAy6V8qEu5JGmySsabpg2JzPwhcDgifroU3Qo8C+wDdpayncDDZXkfsCMilkXEJjonqB8vQ1LHI+LmMqvp9sY2kqQKDfZZ718CX4mIpcD3gX9OJ2D2RsQdwEvAJwEy85mI2EsnSMaAuzJzvOznTuBLwArgkXKTJDXUNLupr5DIzO8CW7usurVH/XuAe7qUDwPXX0D7JGlRqmS0ySuuJUm9GRKSpJ4MCUmqUC2fWmRISJJ6MiQkqTJZ0fQmQ0KSKlTJaJMhIUnqzZCQpMrUM9hkSEhSlSoZbTIkJEm9GRKSVJmKJjcZEpJUIy+mkyRVz5CQpMrMt2+mkyTNsjoGmwwJSdIUDAlJqoyzmyRJU6pkcpMhIUnqzZCQpMpUNNpkSEhSneoYbzIkJEk9GRKSVBlnN0mSpuTsJklS9QwJSapOPeNNhoQkVaiS0SZDQpLUmyEhSZVxdpMkaUrObpIkVc+QkKTKONwkSZpSVDK/yZCQJPVkSEhSZXI+XkwXEa2IeDIivlYer42IRyPi+XK/plF3d0QcioiDEXFbo/zGiHiqrLs3opbz95JUl1reHS/kSOIzwIHG47uB/Zm5GdhfHhMRW4AdwPuAbcDnI6JVtrkP2AVsLrdt76r1kqSLqq+QiIgh4JeALzSKtwN7yvIe4OON8gcyczQzXwAOATdFxHpgdWZ+OzMT+HJjG0lSMR9nN/0O8OtAu1F2VWYeASj3V5byDcDhRr2RUrahLE8uP09E7IqI4YgYPnbsWJ9NlKSFo5LRpulDIiI+ChzNzCf63Ge3vuUU5ecXZt6fmVszc+u6dev6fFpJ0kwb7KPOh4CPRcRHgOXA6oj4A+CViFifmUfKUNLRUn8EuKax/RDwcikf6lIuSWqoaLRp+iOJzNydmUOZuZHOCelvZuangH3AzlJtJ/BwWd4H7IiIZRGxic4J6sfLkNTxiLi5zGq6vbGNJKmhlsmf/RxJ9PJZYG9E3AG8BHwSIDOfiYi9wLPAGHBXZo6Xbe4EvgSsAB4pN0lSpS4oJDLzz4E/L8uvAbf2qHcPcE+X8mHg+gttpCQtJvNxdpMkaREyJCRJPRkSklSZefnZTZKkWZIwUMm7cyXNkCRNaGcyUMkUWENCkirTznn0sRySpNmV4JGEJKm7dua8/D4JSdIsyMxqPpbDkJCkymTCQB0ZYUhIUm2c3SRJ6qldz7V0hoQk1aYz3OSRhCSpi8z0imtJUnftTKKSy+kMCUmqTOdiurluRYchIUmVaWc9X19qSEhSZdIrriVJvTi7SZLUU+diurluRYchIUmVcXaTJKmnTDwnIUnqznMSkqSe/D4JSVJPfjOdJKknjyQkST15xbUkqTevk5Ak9dJ2dpMkqZfOxXR1MCQkqTLpOQlJUi/ObpIk9dTOpFVJShgSklSZ8XbSas2TkIiIayLiWxFxICKeiYjPlPK1EfFoRDxf7tc0ttkdEYci4mBE3NYovzEinirr7o1aBt0kqSLj7fl1JDEG/JvM/DvAzcBdEbEFuBvYn5mbgf3lMWXdDuB9wDbg8xHRKvu6D9gFbC63bTPYF0laEMYzaVVyocS0IZGZRzLzr8ryceAAsAHYDuwp1fYAHy/L24EHMnM0M18ADgE3RcR6YHVmfjszE/hyYxtJUtFuz9PrJCJiI/AB4DHgqsw8Ap0gAa4s1TYAhxubjZSyDWV5cnm359kVEcMRMXzs2LELaaIkzXvj7WRwvhxJTIiIVcAfA7+amT+aqmqXspyi/PzCzPszc2tmbl23bl2/TZSkBWGsnQzMp5CIiCV0AuIrmflQKX6lDCFR7o+W8hHgmsbmQ8DLpXyoS7kkqaGdSauSuaf9zG4K4IvAgcz8XGPVPmBnWd4JPNwo3xERyyJiE50T1I+XIanjEXFz2eftjW0kSUVNs5sG+6jzIeCfAk9FxHdL2b8FPgvsjYg7gJeATwJk5jMRsRd4ls7MqLsyc7xsdyfwJWAF8Ei5SZIa2u2kNVDHocS0IZGZf0H38wkAt/bY5h7gni7lw8D1F9JASVpsxtrzaLhJkjS7xnOenbiWJM2edkXnJAwJSarM2Hy8TkKSdPG1253LxxxukiSdZzw7IeFwkyTpPOMeSUiSepkICc9JSJLOc3a4yZCQJE02Nm5ISJJ6ODk6BsCqZf18atLFZ0hIUkWOn+qExKXLDQlJ0iQnzh5JLJnjlnQYEpJUkROjZwBY5ZGEJGmyieEmz0lIks4zMdzkOQlJ0nlOeCQhSerlxOgYEXDJ0tZcNwUwJCSpKsdPjbFq2SDhB/xJkiY7MTrGpZUMNYEhIUlVOXFqrJrpr2BISFJVToyOVXPSGgwJSarKj06dYdXyOq62BkNCkqqRmbzw6kmuXbtirptyliEhSZX4wWtvcfzUGD911aVz3ZSzDAlJqsSTh98A4KZNa+e4JecYEpJUie8dfpNLlrbYfKVHEpKkSf7P91/j+qsvq+Zb6cCQkKQq/OWhV3nuh8f5pZ9dP9dNeRtDQpIq8J++fpChNSv4J3/vmrluytsYEpI0x7518CjfPfy37PrwdSxfUscH+00wJCRpDr16YpTffPhprr5sOb9807Vz3Zzz1HPttyQtMsMvvs6v/OGTvP7Wab7y6Z9jSau+/9sNCUmaRSdHx9j3vZfZ879f5LkfHufatZfw0J0f5PoNl81107oyJCTpIvvx6XH+1/PH+J/fe5lvPneUt06Ps2X9an7zo1v4xNYhVlf0WU2TGRKS9C6Nt5PXT57mlR+dYuSNHzPyxlu8+NpJRt74MS++epIXX3sLgCtWLeVj77+aT9w4xI3vXVPNFwtNZdZDIiK2Ab8LtIAvZOZnZ7sNkpSZnB5vc3qs3MbbnBwdZ3RsnFNn2pw6M87xU2OcGB3jxKkznDw9zsnRMY6fGuPNH5/hjbdOl2AY5fWTo7Tz7fu/bMUShtas4Gd+YjX/+IYh3n/N5XzwJ99T5XmHqcxqSEREC/gvwC8CI8B3ImJfZj47m+2Q+pGZ5b48bpSfW55Y9/a6U63L5r4n1aWxzURZO5Oxdr5tf5nn9nO2PLusO7vvZnmjXr79uZr7OTOetDNpt5P2pG3a2VnXbEM7O8sT9wmMtZMzY+2z9dvZ+a87Mxkv+51YN96G02Ntxs8+Z55dHm836yWnx9qMtTuvy3i7zdh4p3ysnZw6M86Z8bJ+PBkr60fH2oyOtTkzfi4Uxie/s0+jNRCsXj7I6hVLWLtyKVdeuoyfHbqMK1Yt4z0rl/ITly1naM0lbLh8BWtWLr2gfddqto8kbgIOZeb3ASLiAWA7MOMh8ek93zl7iAfn/iibuv565PR1Ju+re51uu87p6/TxO9tPX/p5/m71+uvL9Pvp3qZ32O5+fnZdthsvb67NN96J52i+4Tc37+f118UXAa0IBgaCVgStgWAgOPt46eAAS1oDDA501g02lpcODnDJ0kEGW8HgQDA4MMBgK1g22GLZkgGWtgbK9sGKJS2WDbZYOjhQtmuxfEmLZYMDLF/S4tLlg6xa1rmtXDbIssGBeTFENJNmOyQ2AIcbj0eAn5tcKSJ2AbsArr32nc0bfu97VrJscNJFKV1+tt1+3JN/CbrXufD9dK3XtU2Tnr/vdk+9n5776qNR/fW3S9k77sv0f4jT9XcgYLAc2k/UnagRca7+5HVEvK1ec9/NumeXJ/++NLbp9ryT153bLrq0sVM+2Iq3tSFo9KFR72x5THr+5rpSfq4PMalfnbqdN9kBBqKzYqCUDwzE2XoD0bgv+xmIc/etAVjSGmCg8YY/8WY/EJ3HMVACIYIlrc4b/WJ7I67ZbIdEt5/8+f8UZt4P3A+wdevWd/S/3b/76JZ3spkkqWG2z6CMAM0PJhkCXp7lNkiS+jTbIfEdYHNEbIqIpcAOYN8st0GS1KdZHW7KzLGI+BXg63SmwP5eZj4zm22QJPVv1q+TyMw/Bf50tp9XknTh5tdVHZKkWWVISJJ6MiQkST0ZEpKknqLbRx7UJCKOAT94h5tfAbw6g82ZT+z74mTfF6dufX9vZq57tzuuPiTejYgYzsytc92OuWDf7ftiY98vTt8dbpIk9WRISJJ6Wughcf9cN2AO2ffFyb4vThet7wv6nIQk6d1Z6EcSkqR3wZCQJPW0IEMiIrZFxMGIOBQRd891e2ZCRFwTEd+KiAMR8UxEfKaUr42IRyPi+XK/prHN7vIaHIyI2xrlN0bEU2XdvTFPvgYsIloR8WREfK08XhR9j4jLI+LBiHiu/PxvWUR9/7Xy+/50RHw1IpYv1L5HxO9FxNGIeLpRNmN9jYhlEfFHpfyxiNjYV8M6X2K+cG50PoL8r4HrgKXA94Atc92uGejXeuCGsnwp8P+ALcB/BO4u5XcD/6Esbyl9XwZsKq9Jq6x7HLiFzjcFPgL8o7nuX5+vwb8G/hD4Wnm8KPoO7AE+XZaXApcvhr7T+brjF4AV5fFe4J8t1L4DHwZuAJ5ulM1YX4F/AfzXsrwD+KO+2jXXL8xFeKFvAb7eeLwb2D3X7boI/XwY+EXgILC+lK0HDnbrN53v8Lil1HmuUf7LwH+b6/700d8hYD/w85wLiQXfd2B1eaOMSeWLoe8bgMPAWjpfa/A14B8u5L4DGyeFxIz1daJOWR6kc4V2TNemhTjcNPGLNWGklC0Y5TDxA8BjwFWZeQSg3F9ZqvV6HTaU5cnltfsd4NeBdqNsMfT9OuAY8PtlqO0LEbGSRdD3zPwb4LeBl4AjwJuZ+Q0WQd8bZrKvZ7fJzDHgTeA90zVgIYZEt7HGBTPPNyJWAX8M/Gpm/miqql3KcoryakXER4GjmflEv5t0KZuXfafzH98NwH2Z+QHgJJ1hh14WTN/L+Pt2OsMpVwMrI+JTU23SpWxe9r0P76Sv7+h1WIghMQJc03g8BLw8R22ZURGxhE5AfCUzHyrFr0TE+rJ+PXC0lPd6HUbK8uTymn0I+FhEvAg8APx8RPwBi6PvI8BIZj5WHj9IJzQWQ99/AXghM49l5hngIeCDLI6+T5jJvp7dJiIGgcuA16drwEIMie8AmyNiU0QspXOCZt8ct+ldKzMUvggcyMzPNVbtA3aW5Z10zlVMlO8oMxo2AZuBx8sh6/GIuLns8/bGNlXKzN2ZOZSZG+n8PL+ZmZ9icfT9h8DhiPjpUnQr8CyLoO90hplujohLSptvBQ6wOPo+YSb72tzXJ+j8HU1/RDXXJ2ou0smfj9CZ/fPXwG/MdXtmqE9/n86h4f8FvltuH6EzprgfeL7cr21s8xvlNThIYzYHsBV4uqz7z/Rx8qqWG/APOHfielH0Hfi7wHD52f8JsGYR9f3fA8+Vdv93OrN5FmTfga/SOfdyhs5//XfMZF+B5cD/AA7RmQF1XT/t8mM5JEk9LcThJknSDDEkJEk9GRKSpJ4MCUlST4aEJKknQ0KS1JMhIUnq6f8D6fG/q9YPIH8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    1    5    5   15   16   17   23   26\n",
      "   30   30   34   34   40   40   46   47   48   50   50   51   54   54\n",
      "   55   61   62   64   67   68   69   69   73   73   74   75   77   77\n",
      "   78   79   79   80   81   83   85   86   87   88   89   89   90   92\n",
      "   93   95   96   97   97   97   98   99  102  104  105  106  106  107\n",
      "  107  108  109  110  110  110  110  110  111  112  112  112  113  115\n",
      "  115  117  117  117  118  118  119  119  120  120  121  121  121  121\n",
      "  121  121  122  122  122  123  123  125  125  126  126  127  127  128\n",
      "  129  129  130  130  130  131  131  132  132  133  133  134  134  135\n",
      "  136  136  137  138  139  139  139  140  142  142  143  144  144  144\n",
      "  145  145  145  145  146  147  147  147  149  149  150  150  151  151\n",
      "  151  152  153  153  153  153  154  154  154  155  155  156  157  157\n",
      "  158  158  158  159  159  159  160  160  160  161  161  161  161  161\n",
      "  161  161  162  162  162  162  163  164  164  165  165  166  167  167\n",
      "  167  167  167  168  169  169  169  169  170  171  171  171  172  172\n",
      "  172  172  172  172  173  173  174  174  174  175  175  175  175  175\n",
      "  176  176  176  177  177  177  178  178  179  179  179  179  180  180\n",
      "  180  180  180  181  182  182  182  183  183  184  185  185  185  185\n",
      "  186  186  187  187  188  188  188  188  188  189  189  189  190  191\n",
      "  191  191  192  192  192  192  192  192  193  193  193  193  194  195\n",
      "  195  195  195  195  195  196  196  196  197  197  198  198  198  199\n",
      "  199  200  200  200  200  200  201  201  201  202  202  203  204  204\n",
      "  204  204  204  205  205  205  205  206  206  207  208  208  208  208\n",
      "  209  211  211  211  212  212  212  213  213  213  214  214  214  215\n",
      "  215  215  215  215  215  215  216  216  216  217  217  217  217  217\n",
      "  218  218  219  219  219  219  219  220  220  221  221  221  221  222\n",
      "  222  222  222  222  223  223  223  223  224  224  224  224  224  225\n",
      "  225  225  225  226  226  226  226  226  227  228  228  229  229  230\n",
      "  230  230  230  230  230  231  231  232  232  232  232  233  233  233\n",
      "  234  234  235  235  235  235  235  236  236  236  236  236  236  237\n",
      "  237  238  238  238  239  239  239  239  239  239  240  240  240  241\n",
      "  242  243  244  245  245  245  246  246  246  246  246  246  246  246\n",
      "  247  247  247  247  247  248  248  249  250  250  250  251  251  251\n",
      "  251  252  252  252  252  252  252  253  253  253  255  255  256  256\n",
      "  256  257  257  257  257  257  258  259  259  260  260  260  260  260\n",
      "  261  261  261  261  261  262  262  262  262  263  263  263  263  264\n",
      "  265  265  265  266  266  266  267  267  267  268  268  268  268  269\n",
      "  269  270  271  271  271  272  272  273  273  274  274  275  275  276\n",
      "  276  277  277  278  278  278  279  279  279  280  280  280  282  282\n",
      "  282  282  283  283  283  284  284  284  284  284  284  284  284  285\n",
      "  285  285  285  286  287  287  288  288  288  289  290  290  291  291\n",
      "  291  292  292  292  293  293  293  295  296  296  297  297  297  299\n",
      "  299  299  299  299  299  300  300  302  302  302  303  303  304  304\n",
      "  305  307  307  307  308  308  308  308  308  308  309  309  309  310\n",
      "  310  310  310  311  313  313  313  313  313  315  315  315  315  315\n",
      "  315  316  317  317  318  318  319  319  320  320  321  321  321  321\n",
      "  321  322  323  323  324  324  326  326  326  327  327  327  327  327\n",
      "  328  328  328  328  328  328  328  329  329  330  330  330  330  330\n",
      "  330  330  331  332  332  332  333  334  334  334  335  336  337  337\n",
      "  338  340  340  341  341  341  341  341  342  342  342  342  342  343\n",
      "  343  344  344  344  344  344  345  345  346  346  348  349  349  349\n",
      "  350  350  351  351  352  352  356  357  357  358  358  359  359  359\n",
      "  362  362  363  364  364  365  366  366  368  368  368  368  369  369\n",
      "  370  370  370  370  371  373  373  374  374  375  375  375  375  378\n",
      "  378  378  379  380  380  382  382  382  382  383  384  385  385  385\n",
      "  386  386  388  388  390  391  392  392  392  392  393  393  395  395\n",
      "  396  396  397  397  398  398  399  401  402  402  405  405  405  406\n",
      "  406  407  407  408  408  409  409  409  410  411  412  415  415  416\n",
      "  417  417  420  420  421  421  421  423  424  424  424  426  427  427\n",
      "  428  429  429  430  431  431  431  433  434  434  435  436  436  437\n",
      "  437  438  439  441  442  443  444  445  445  445  449  451  451  451\n",
      "  452  452  453  453  454  457  458  461  462  463  465  467  468  468\n",
      "  469  472  472  473  473  474  475  475  475  475  475  475  477  478\n",
      "  478  479  482  483  484  485  492  493  496  497  497  498  501  502\n",
      "  502  502  505  508  511  511  511  512  513  515  516  518  518  519\n",
      "  521  524  527  531  533  533  533  534  539  539  540  541  545  545\n",
      "  546  547  553  555  556  558  559  560  561  564  566  569  571  572\n",
      "  572  577  579  579  580  588  590  592  594  597  606  608  610  613\n",
      "  613  617  626  630  654  654  658  662  665  667  677  679  687  707\n",
      "  714  731  734  795  806  832  867  877  887  916  968 1078 1089 1155\n",
      " 1195 1209 1677 1758 1762 2617]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "def model():\n",
    "    \n",
    "    data = open_json_data()\n",
    "    data = fill_Nan(data)\n",
    "    data = get_ref(data, False)\n",
    "    data = get_years(data, False)    \n",
    "    data = get_titleLen(data)\n",
    "    data = get_FieldsOfStudyLen(data)\n",
    "    data = get_TopicsLen(data)\n",
    "    data = get_AuthorsLen(data)\n",
    "    \n",
    "    print(data.shape)\n",
    "\n",
    "    data_test = open_json_data('test.json')\n",
    "    data_test = fill_Nan(data_test)\n",
    "    data_test = get_ref(data_test, False)\n",
    "    data_test = get_years(data_test, False)    \n",
    "    data_test = get_titleLen(data_test)\n",
    "    data_test = get_FieldsOfStudyLen(data_test)\n",
    "    data_test = get_TopicsLen(data_test)\n",
    "    data_test = get_AuthorsLen(data_test)\n",
    "\n",
    "    print(data_test.shape)\n",
    "\n",
    "    \n",
    "    X_train = data.drop([\"citations\", \"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "    y_train = data[\"citations\"].values\n",
    "    plotY(y_train)\n",
    "\n",
    "    X_test = data_test.drop([\"year\", \"doi\", \"title\", \"abstract\", \"authors\", \"topics\",\"fields_of_study\", \"venue\"], axis=1).values\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), linear_model.Lasso(alpha=1.1))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    Ypred = pd.Series(predictions.astype(int))\n",
    "    Ypred = Ypred.where(Ypred > 0, 0)\n",
    "    \n",
    "    data_test[\"citations\"] = predictions.astype('int') \n",
    "    \n",
    "    # Ypred = np.array(Ypred)\n",
    "    YpredSorted = np.sort(Ypred)\n",
    "    print(YpredSorted)\n",
    "\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "model().to_json(\"output.json\", orient='records')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
